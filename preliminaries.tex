
Let $\bbfZ$ and $\bbfZ^{\geq 0}$ be the sets of integers and non-negative integers respectively.
% For $n \geq 0$, $\underline{n} = \{ 0, 1, \ldots, n \} \subseteq
% \bbfZ^{\geq 0}$.
%\lz{not so often used: can we do without this notation?}
We briefly review the definitions of differential privacy, Markov
chains, and Markov decision processes~\cite{Put05}. For differential privacy, we
follow the standard definition in~\cite{DR:14:AFDP,GRS:09:UUPM,GRS:12:UUPM}. Our
definitions of Markov chains and Markov decision processes are adopted
from~\cite{BK:08:PMC}.

\subsection{Differential Privacy}
We denote the data universe by $\mathcal{X}$; $x \in \mathcal{X}^n$ is
a \emph{dataset} with $n$ \emph{rows} from the data universe. Two datasets $x$ and
$x'$ are \emph{neighbors} (denoted by $d(x, x') \leq 1$) if they are
identical except at most one row. A \emph{query} $f$ is a function
from $\mathcal{X}^n$ to its range $R \subseteq \bbfZ$. The \emph{sensitivity} of the
query $f$ (written $\Delta (f)$) is $\max_{d(x, x') \leq 1} | f (x) -
f (x') |$. For instance, a \emph{counting} query counts the number
of rows with certain attributes (say, female). The sensitivity of a
counting query is $1$ since any neighbor can change the count by at
most one. We only consider queries with finite  ranges for simplicity.
A \emph{data analysis mechanism} (or
\emph{mechanism} for brevity) $M_f$ for a query $f$
is a randomized algorithm with inputs in $\mathcal{X}^n$ and outputs
in $\tilde{R}$.
A mechanism may not have the same output range as its query, that is,
$\tilde{R} \neq R$ in general.
A mechanism $M_f$ for $f$ is \emph{oblivious} if
$\Pr[M_f(x) = \tilde{r}] = \Pr[M_f(x') = \tilde{r}]$ for every
$\tilde{r} \in \tilde{R}$ when $f (x) = f (x')$. In words, outputs of an
oblivious mechanism depend on the query result $f (x)$.
The order of rows, for instance, is
irrelevant to oblivious mechanisms. Let $x, x'$
be datasets and $\tilde{r} \in \tilde{R}$. The probability of
the mechanism $M_f$ outputting $\tilde{r}$ on $x$ is
\emph{$(\epsilon, \delta)$-close} to those on $x'$ if
\[
\Pr[M_f (x) = \tilde{r}] \leq e^{\epsilon} \Pr[M_f (x') =
\tilde{r}] + \delta.
%\footnote{The standard definition requires
%$\Pr[M_f (x) = \tilde{r}] \leq e^\epsilon \Pr[M_f (x') =
%\tilde{r}] + \delta$~\cite{DMNS:06:CNSPD,D:06:DP}.
%Since $\epsilon = e^{\ln \epsilon}$, our $(\epsilon, \delta)$-closedness is
%equivalent to the standard $(\ln \epsilon,
%\delta)$-closedness~\cite{GRS:09:UUPM,GRS:12:UUPM}.}
\]
A mechanism $M_f$ is \emph{$(\epsilon, \delta)$-differentially
  private} % $(\epsilon, \delta \approx 0)$
if for every $x, x' \in \mathcal{X}^n$ with $d(x, x') \leq 1$
and $\tilde{r} \in \tilde{R}$, the probability of
$M_f$ outputting $\tilde{r}$ on $x$ is $(\epsilon, \delta)$-close to
those on $x'$.

The non-negative parameters $\epsilon$ and $\delta$ quantify
mechanism behaviors probabilistically;
the smaller they are, the behaviors are more similar.
Informally, a differentially private mechanism has probabilistically
similar behaviors on neighbors. It will have similar
output distributions when any row is replaced by another in a given
dataset. 
%Consider, for instance, a database contains the row
%corresponding to an individual. A differentially private mechanism
%will have a similar output distribution when the individual is not in
%the database. Such mechanisms behave similarly regardless of the
%presence of any individual. 
Since the output distribution does not change significantly with the
absence of any row in a dataset,
individual privacy is thus preserved by differentially private mechanisms.

\subsection{Markov Chains and Markov Decision Processes}

Let $\AP$ be the set of \emph{atomic propositions}.
A \emph{(finite) discrete-time Markov chain} $K = (S, \wp, L)$ consists
of a non-empty finite set $S$ of \emph{states}, a \emph{transition
  probability function} $\wp : S \times S \rightarrow [0, 1]$ with
$\sum_{t \in S} \wp(s, t) = 1$ for every $s \in S$, and
a \emph{labeling function} $L : S \rightarrow 2^{\AP}$. A \emph{path}
in $K$ is an infinite sequence $\pi = \pi_0 \pi_1 \cdots \pi_n \cdots$
of states with $\wp (\pi_i, \pi_{i+1}) > 0$ for all $i \geq 0$. We write
$\pi[j]$ for the suffix $\pi_j \pi_{j+1} \cdots$. % Thus $\pi[0] = \pi$.

A \emph{(finite) Markov decision process}
(MDP)~\footnote{The MDP we consider is \emph{reactive} in the sense that
all actions are enabled in every state.
% If equipped with a set of final states $F\subseteq S$, such model is
% referred to as \emph{probabilistic automaton} in the
% literature~\cite{Rabin63}, and language equivalence/inclusion
% problems have been investigated. In the model checking context, the
% notion MDPs is mostly used.
}
$M = (S, \Act, \wp, L)$ consists
of
a finite set of \emph{actions} $\Act$,
a \emph{transition probability function} $\wp : S \times \Act
\times S \rightarrow [0, 1]$ with $\sum_{t \in S} \wp(s, \alpha, t)
= 1$ for every $s \in S$ and $\alpha \in \Act$. $S$ and
$L$ are as for Markov chains.
A \emph{path} $\pi$ in $M$ is an infinite sequence $\pi_0 \alpha_1
\pi_1 \cdots \pi_n \alpha_{n+1} \cdots$ with
$\wp(\pi_i, \alpha_{i+1}, \pi_{i+1}) > 0$ for all $i \geq 0$.
Similarly, we write $\pi[j]$ for the suffix $\pi_j \alpha_{j+1}
\pi_{j+1} \cdots$ of $\pi$.
\hide{
For any measurable set $\Pi$ of paths, we
write $\Pr[\Pi]$ for the \emph{probability measure} of $\Pi$.
}

Let $M = (S, \Act, \wp, L)$ be an MDP. A (history-dependent)
\emph{scheduler} for $M$ is a function $\scheduler{S} : S^+
\rightarrow \Act$. A \emph{query scheduler} for $M$ is a function
$\scheduler{Q} : S^+ \rightarrow \Act$ such that
$\scheduler{Q}(\sigma) = \scheduler{Q}(\sigma')$ for any $\sigma,
\sigma' \in S^+$ of the same length.
Intuitively, decisions of a query scheduler depend only on the length of the history.
A path $\pi =
\pi_0 \alpha_1 \pi_1 \cdots$ $\pi_n$ $\alpha_{n+1} \cdots$ is an
\emph{$\scheduler{S}$-path} if $\alpha_{i+1} = \scheduler{S}(\pi_0
\pi_1 \cdots \pi_i)$ for all $i \geq 0$.
Note that an MDP with a
scheduler $\scheduler{S}$ induces a Markov chain $M_{\scheduler{S}} =
(S^+, \wp_{\scheduler{S}}, L')$ where $L' (\sigma s) = L (s)$,
$\wp_{\scheduler{S}} (\sigma s, \sigma s t) = \wp (s,
\scheduler{S}(\sigma s), t)$ for $\sigma \in S^*$ and $s, t \in S$.
\hide{
Subsequently, we write
$\Pr_{\scheduler{S}} (\Pi)$ for the \emph{probability measure} of the
measurable path set $\Pi$ on $M$ given the scheduler $\scheduler{S}$.
}

\hide{
A \emph{reward function} on an MDP $M = (S, \wp, I, L)$ is a function
$r : S \rightarrow \bbfR$. The \emph{reward} for $r (\pi)$ a finite
path $\pi$ in $M$ is $\sum_{i=0}^n s_i$ where $\pi = s_0 \alpha_1 s_1
\cdots s_n$.
}
