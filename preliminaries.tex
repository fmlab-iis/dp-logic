
Let $\bbfZ$ and $\bbfZ^{\geq 0}$ be the sets of integers and
non-negative integers respectively.
% For $n \geq 0$, $\underline{n} = \{ 0, 1, \ldots, n \} \subseteq
% \bbfZ^{\geq 0}$.
%\lz{not so often used: can we do withuot this notatin?}
We briefly review the definitions of differential privacy, Markov
chains, and Markov decision processes~\cite{Put05}. For differential privacy, we
follow the standard definition in~\cite{DR:14:AFDP,GRS:09:UUPM,GRS:12:UUPM}. Our
definitions of Markov chains and Markov decision processes are adopted
from~\cite{BK:08:PMC}.

\subsection{Differential Privacy}
We denote the data universe by $\mathcal{X}$; $x \in \mathcal{X}^n$ is
a database with $n$ rows from the data universe. Two databases $x$ and
$x'$ are \emph{neighbors} (denoted by $d(x, x') \leq 1$) if they are
identical except at most one row. A \emph{query} $f$ is a function
from $\mathcal{X}^n$ to its range $R$. The \emph{sensitivity} of the
query $f$ (written $\Delta (f)$) is $\max_{d(x, x') \leq 1} | f (x) -
f (x') |$. For instance, a \emph{counting} query counts the number
of rows with certain attributes (say, female). The sensitivity of a
counting query is $1$ since any neighbor can change the count by at
most one. We only consider queries with finite  ranges for simplicity.
A \emph{data analysis mechanism} (or
\emph{mechanism} for brevity) $M_f$ for a query $f$
is a randomized algorithm with inputs in $\mathcal{X}^n$ and outputs
in $\tilde{R}$.
A mechanism may not have the same output range as its query, that is,
$\tilde{R} \neq R$ in general.
A mechanism $M_f$ for $f$ is \emph{oblivious} if
$\Pr[M_f(x) = \tilde{r}] = \Pr[M_f(x') = \tilde{r}]$ for every
$r \in \tilde{R}$ when $f (x) = f (x')$. In words, outputs of an
oblivious mechanism depend on the query result $f (x)$ but not on the
input database $x$. The order of rows in a database, for instance, is
irrelevant to oblivious mechanisms. A database $x$ is
\emph{$(\epsilon, \delta)$-close} to $x'$ in a mechanism
$M_f$ if for every $\tilde{r} \in \tilde{R}$,
\[
\Pr[M_f (x) = \tilde{r}] \leq e^{\epsilon} \Pr[M_f (x') =
\tilde{r}] + \delta.
%\footnote{The standard definition requires
%$\Pr[M_f (x) = \tilde{r}] \leq e^\epsilon \Pr[M_f (x') =
%\tilde{r}] + \delta$~\cite{DMNS:06:CNSPD,D:06:DP}.
%Since $\epsilon = e^{\ln \epsilon}$, our $(\epsilon, \delta)$-closedness is
%equivalent to the standard $(\ln \epsilon,
%\delta)$-closedness~\cite{GRS:09:UUPM,GRS:12:UUPM}.}
\]
A mechanism $M_f$ is \emph{$(\epsilon, \delta)$-differentially
  private} % $(\epsilon, \delta \approx 0)$
if for every $x, x' \in \mathcal{X}^n$ with $d(x, x') \leq 1$,
$x$ is $(\epsilon, \delta)$-close to $x'$ in $M_f$.

The parameters $\epsilon$ and $\delta$ quantify probabilistically
similar behaviors;
the smaller they are, the behaviors are more similar.
Informally, a differentially private mechamism has probabilistically
similar behaviors on neighboring databases. It will have similar
output distributions when a row is replaced by another in a given
database. Consider, for instance, a database contains the row
corresponding to an individual. A differentially private mechanism
will have a similar output distribution when the individual is not in
the database. Such mechanisms behave similarly regardless of the
presence of any individual. Individual privacy is thus
preserved by differentially private mechanisms.

\subsection{Markov Chains and Markov Decision Processes}

Let $\AP$ be the set of \emph{atomic propositions}.
A \emph{(finite) discrete-time Markov chain} $K = (S, \wp, L)$ consists
of a non-empty finite set $S$ of \emph{states}, a \emph{transition
  probability function} $\wp : S \times S \rightarrow [0, 1]$ with
$\sum_{t \in S} \wp(s, t) = 1$ for every $s \in S$, and
a \emph{labeling function} $L : S \rightarrow 2^{\AP}$. A \emph{path}
in $K$ is an infinite sequence $\pi = \pi_0 \pi_1 \cdots \pi_n \cdots$
of states with $\wp (\pi_i, \pi_{i+1}) > 0$ for all $i \geq 0$. We write
$\pi[j]$ for the suffix $\pi_j \pi_{j+1} \cdots$. % Thus $\pi[0] = \pi$.

A \emph{(finite) Markov decision process}
(MDP)~\footnote{The MDP we consider is \emph{reactive} in the sense that
all actions are enabled in every state. If equipped with a set of final states $F\subseteq S$, such model is
referred to as \emph{probabilistic automaton} in the literature~\cite{Rabin63}, and language equivalence/inclusion
problems have been investigated. In the model checking context, the notion MDPs is mostly used.}
 $M = (S, \Act, \wp, L)$ consists
of
a finite set of \emph{actions} $\Act$,
a \emph{transition probability function} $\wp : S \times \Act
\times S \rightarrow [0, 1]$ with $\sum_{t \in S} \wp(s, \alpha, t)
= 1$ for every $s \in S$ and $\alpha \in \Act$. $S$ and
$L$ are as for Markov chains.
A \emph{path} $\pi$ in $M$ is an infinite sequence $\pi_0 \alpha_1
\pi_1 \cdots \pi_n \alpha_{n+1} \cdots$ with
$\wp(\pi_i, \alpha_{i+1}, \pi_{i+1}) > 0$ for all $i \geq 0$.
Similarly, we write $\pi[j]$ for the suffix $\pi_j \alpha_{j+1}
\pi_{j+1} \cdots$ of $\pi$.
\hide{
For any measurable set $\Pi$ of paths, we
write $\Pr[\Pi]$ for the \emph{probability measure} of $\Pi$.
}

Let $M = (S, \Act, \wp, L)$ be an MDP. A (history-dependent)
\emph{scheduler} for $M$ is a function $\scheduler{S} : S^+
\rightarrow \Act$. A \emph{query scheduler} for $M$ is a function
$\scheduler{Q} : \bbfZ^{\geq 0} \rightarrow \Act$. Intuitively, its decision depends only the length of the history.
A path $\pi =
\pi_0 \alpha_1 \pi_1 \cdots$ $\pi_n$ $\alpha_{n+1} \cdots$ is an
\emph{$\scheduler{S}$-path} if $\alpha_{i+1} = \scheduler{S}(\pi_0
\pi_1 \cdots \pi_i)$ for all $i \geq 0$.
Note that an MDP with a
scheduler $\scheduler{S}$ induces a Markov chain $M_{\scheduler{S}} =
(S^+, \wp_{\scheduler{S}}, L')$ where $L' (\sigma s) = L (s)$,
$\wp_{\scheduler{S}} (\sigma s, \sigma s t) = \wp (s,
\scheduler{S}(\sigma s), t)$ for $\sigma \in S^*$ and $s, t \in S$.
\hide{
Subsequently, we write
$\Pr_{\scheduler{S}} (\Pi)$ for the \emph{probability measure} of the
measurable path set $\Pi$ on $M$ given the scheduler $\scheduler{S}$.
}

\hide{
A \emph{reward function} on an MDP $M = (S, \wp, I, L)$ is a function
$r : S \rightarrow \bbfR$. The \emph{reward} for $r (\pi)$ a finite
path $\pi$ in $M$ is $\sum_{i=0}^n s_i$ where $\pi = s_0 \alpha_1 s_1
\cdots s_n$.
}
