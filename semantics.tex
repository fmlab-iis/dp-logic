
Given a finite discrete-time Markov chain $K = (S, \wp, L)$, a
\emph{neighborhood relation on $S$} $\neighbor{S} \subseteq S \times S$
is a reflexive and symmetric relation on $S$. We will write $s
\neighbor{S} t$ when $(s, t) \in \neighbor{S}$. If $s \neighbor{S} t$,
we say $s$ and $t$ are \emph{neighbors} or $t$ is a \emph{neighbor} of
$s$. Define the satisfaction relation $K, \neighbor{S}, s
\models \Phi$ as follows.
\begin{eqnarray*}
  K, \neighbor{S}, s \models \top\\
  K, \neighbor{S}, s \models p
  & \textmd{ if } &
  p \in L(s)\\
  K, \neighbor{S}, s \models \neg \Phi
  & \textmd{ if } &
  K, \neighbor{S}, s \not\models \Phi\\
  K, \neighbor{S}, s \models \Phi_0 \wedge \Phi_1
  & \textmd{ if } &
  K, \neighbor{S}, s \models \Phi_0 \textmd{ and }
  K, \neighbor{S}, s \models \Phi_1\\
  K, \neighbor{S}, s \models \PJ{J} \phi
  & \textmd{ if } &
  \Pr [\{ \pi : K, \neighbor{S}, \pi \models \phi \textmd{ with }
                    \pi_0 = s\}] \in J \\
  K, \neighbor{S}, s \models \dpriv{\epsilon}{\delta} \phi
  & \textmd{ if } &
  \textmd{for every } t \textmd{ with }  s \neighbor{S} t,
      p(s) \leq \epsilon p(t) + \delta \textmd{ and }
      p(t) \leq \\
  & & \epsilon p(s)  + \delta  \textmd{ where } p(x) = \Pr [\{
      \pi : K, \neighbor{S}, \pi \models \phi \textmd{ with}\\
  & &  \pi_0 = x \}]
\end{eqnarray*}

Moreover, the relation $K, \neighbor{S}, \pi \models \phi$ is defined as
for the standard LTL formulas. We only give the semantics for $\X$ and $\until$ as follows.
\begin{eqnarray*}
  K, \neighbor{S}, \pi \models \X \phi
  & \textmd{ if } &
  K, \neighbor{S}, \pi[1] \models \phi\\
  K, \neighbor{S}, \pi \models \phi \until \psi
  & \textmd{ if } &
  \textmd{there is a } j \geq 0 \textmd{ such that }
  K, \neighbor{S}, \pi[j] \models \psi \textmd{ and } \\
  & & K, \neighbor{S}, \pi[k] \models \phi
      \textmd{ for every } 0 \leq k < j
\hide{
  K, \neighbor{S}, \pi \models \Phi \buntil{n} \Psi
  & \textmd{ if } &
  \textmd{there is a } 0 \leq j \leq n \textmd{ such that }
  K, \neighbor{S}, \pi[j] \models \Psi \textmd{ and } \\
  & & K, \neighbor{S}, \pi[k] \models \Phi
      \textmd{ for every } 0 \leq k < j
}
\end{eqnarray*}

To simplify our notation, for any Markov chain $K$, neighborhood
relation $N$ on $S$, a state $s \in S$, and a path formula $\phi$,
define
\[
\myPr{s}{K}{N}{\phi} =
\Pr [ \{ \pi : K, N, \pi \models \phi \textmd{ with } \pi_0 = s \} ].
\]
That is, $\myPr{s}{K}{N}{\phi}$ denotes the probability of paths
satisfying $\phi$ from $s$ on $K$ with $N$. The semantics of the
differentially private operator hence becomes
\begin{eqnarray*}
  K, \neighbor{S}, s \models \dpriv{\epsilon}{\delta} \phi
  & \textmd{ if } &
  \textmd{for every } t \textmd{ with }  s \neighbor{S} t,
      \myPr{s}{K}{N}{\phi} \leq \epsilon \myPr{t}{K}{N}{\phi} + \delta
      \textmd{ and }\\
  & &  \myPr{t}{K}{N}{\phi} \leq \epsilon \myPr{s}{K}{N}{\phi} + \delta
\end{eqnarray*}

Other than the differentially private operator, the semantics of
$\dpCTL$ is standard~\cite{BK:08:PMC}.
To intuit the semantics of $\dpriv{\epsilon}{\delta} \phi$,
recall that  $\myPr{s}{K}{N}{\phi}$ is the probability of having
paths satisfying $\phi$ from $s$. Thus, a state $s$ satisfies
$\dpriv{\epsilon}{\delta} \phi$ if having paths satisfying $\phi$
from $s$ is $(\epsilon, \delta)$-close to having paths satisfying
$\phi$ from every neighbor of $s$. In other words, it is
probabilistically similar to observe paths satisfying $\phi$ from $s$
and from all neighbors of $s$.

\section{Model Checking Algorithms}
The model checking algorithms for $\dpCTL$ follow the classical
algorithms for $\PCTL$ by computing the states satisfying
sub state-formulae inductively~\cite{BK:08:PMC}. It hence suffices to
consider the inductive step where the states satisfying the subformula
$\dpriv{\epsilon}{\delta} (\phi)$ is computed.

%We start with the algorithm for Markov chains.

In the classical $\PCTL$ model checking algorithm for Markov chains,
states satisfying the subformula $\PJ{J} \phi$ are obtained by
computing $\myPr{s}{K}{\neighbor{S}}{\phi}$ for state $s \in S$.
These probabilities can be computed by solving linear equations or
through iterative approximations. We summarize it in the following
theorem~\cite{BK:08:PMC}:

\begin{lemma}
  Let $K = (S, \wp, L)$ be a Markov chain, $s
  \in S$, and $B, C \subseteq S$. The probabilities
  $\myPr{s}{K}{\neighbor{S}}{\gX B}$ and
  $\myPr{s}{K}{\neighbor{S}}{B \mho C}$
\hide{
  $\Pr[\{ \pi : K, \neighbor{K}, \pi \models B \buntil{n} C \textmd{
    with } \pi_0 = s \}]$
} are computable within time polynomial in
  $|S|$.
  \label{lemma:PJ-subroutines}
\end{lemma}

In Lemma~\ref{lemma:PJ-subroutines}, we abuse the notation slightly to
admit path formulae of the form $\gX B$ (next $B$) and
$B \guntil C$ ($B$ until $C$) with $B, C \subseteq S$ as
in~\cite{BK:08:PMC}. They are interpreted by introducing new atomic
propositions $B$ and $C$ for each $s \in B$ and $s \in C$
respectively. We will also use graphical symbols $\gF C$ ($C$ in
the future) and $\gG C$ (globally $C$) in such path formulae
$\varphi$ to minimize confusion.

In order to determine the set $\{ s : K, \neighbor{S}, s \models
\dpriv{\epsilon}{\delta} \varphi \}$, our algorithm first computes
the probabilities $p (s) = \myPr{s}{K}{\neighbor{S}}{\varphi}$ for every
$s \in S$ (Algorithm~\ref{algorithm:sat-dpriv-dtmc}). For each $s \in S$,
it then compares the probabilities $p (s)$ and $p (t)$ for every
neighbor $t$ of $s$. If there is a neighbor $t$ such that $p (s)$ and
$p (t)$ are not $(\epsilon, \delta)$-close, the state $s$
is removed from the result. Hence
Algorithm~\ref{algorithm:sat-dpriv-dtmc} returns all states which are
$(\epsilon, \delta)$-close to their neighbors.
The algorithm requires at most $O
(|S|^2)$ additional steps. We hence have the following results:

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{SAT}{$K$, $\neighbor{S}$, $\phi$}
    \Match{$\phi$}
    \Comment{by Lemma~\ref{lemma:PJ-subroutines}}
    \Case{$\X \Psi$}
      \State{$B \leftarrow \textmd{SAT} (K, \neighbor{S}, \Psi)$}
      \State{$p(s) \leftarrow \myPr{s}{K}{\neighbor{S}}{\gX B}$
        for every $s \in S$}
    \EndCase
    \Case{$\Psi \until \Psi'$}
      \State{$B \leftarrow \textmd{SAT} (K, \neighbor{S}, \Psi)$}
      \State{$C \leftarrow \textmd{SAT} (K, \neighbor{S}, \Psi')$}
      \State{$p(s) \leftarrow \myPr{s}{K}{\neighbor{S}}{B \guntil C}$ for
        every $s \in S$}
    \EndCase
    \EndMatch
    \State{$R \leftarrow S$}
    \For{$s \in S$}
      \For{$t$ with $s \neighbor{K} t$}
        \If{$p(s) \not\leq \epsilon p(t) + \delta$ or
            $p(t) \not\leq \epsilon p(s) + \delta$}
        {remove $s$ from $R$}
        \EndIf
      \EndFor
    \EndFor

    \State{\Return $R$}
    \EndFunction
  \end{algorithmic}
  \caption{SAT($\phi$, $\neighbor{K}$)}
  \label{algorithm:sat-dpriv-dtmc}
\end{algorithm}

\begin{proposition}
  Let $K = (S, \wp, L)$ be a Markov chain.
  $\{ s : K, \neighbor{K}, s \models \dpriv{\epsilon}{\delta} \phi \}$ is
  computable within time polynomial in $|S|$.
\end{proposition}

\begin{corollary}
  Let $K = (S, \wp, L)$ be a Markov chain and $\Phi$ a $\dpCTL$
  formula. $\{ s : K, \neighbor{K}, s \models \Phi \}$ is
  computable within time polynomial in $|S|$ and $|\Phi|$.
\end{corollary}


\section{Specifying Properties in $\dpCTL$}
In this section we describe what type of properties in the differential privacy context can be expressed using $\dpCTL$.

\paragraph{Standard Differential Privacy.} 
The formula $\dpriv{3}{0} (\X Y)$ holds in state $s$ if for  $t$ with $sNt$, the probability of satisfying $\X Y$ from $s$ and $t$ satisfy:
$\myPr{s}{K}{N}{\X Y}\le 3\myPr{t}{K}{N}{\X Y}$. The formula $\dpriv{3}{0} (\X Y)\wedge \dpriv{3}{0} (\X N)$ thus specifies the differential privacy property for states $+$ and $-$ in Example~\ref{exa:survey}.

Further, consider the Markov chain in
Figure~\ref{figure:geometric-mechanism-markov-chain}. Assume
$L(out_i) = \{ out_i \}$ and $\Delta (f) = 1$. Then $in_k$ and $in_l$ are
neighbors iff $| k - l | \leq 1$\lz{put above sentence to the def of the model}. Define the $\dpCTL$ formula
$\psi = \dpriv{.5}{0} (\F out_0) \wedge \dpriv{.5}{0} (\F out_1) \wedge
\cdots \wedge \dpriv{.5}{0} (\F out_5)$\lz{I think we better use $\X$ operator here ;)}. If the state $in_k$ satisfies
$\psi$ for $k = 0, \ldots, 5$, then the $.5$-geometric mechanism is
$(0.5, 0)$-differentially private.


\paragraph{Compositional Properties.}
Compositional aspect is one of the building block for differentially private algorithms. According to the compositional theorem \cite{}, for $(\epsilon_1,0)$-differentially private algorithm $M_1$ and
$(\epsilon_2,0)$-differentially private algorithm $M_2$, their combination $M$ defined by $M(x)=(M_1(x), M_2(x))$ is $(\epsilon_1\epsilon_2,0)$-differentially private. The degradation is rooted in the repeated computation. To illustrate this property, we consider the extended example below \lz{here example of the survey example, expanded twice: we should discuss the graph before it is drawn...}

We consider the formula $\dpriv{9}{0} (\X (Y\wedge \X Y))$. A path satisfies $\X (Y\wedge \X Y)$ if the second state satisfies $Y$ and the third state satisfies $Y$ as well. We verify that this formula is satisfied  for state $+$ and $-$. Moreover, the bound $\epsilon=9$ is bound, since \lz{tbd once graph is given}. Finally, the formula $\wedge_{a_1,a_2}\dpriv{9}{0} (\X (a_1\wedge \X a_2))$ corresponds to the desired compositional property for the model, where $a_1,a_2$ range over all atomic propositions $\{Y,N\}$.

We consider another simpler formula for comparision: $\dpriv{3}{0} (\X \X Y)$. In this case we donot have privacy lost, even though we are querying twice: the reason is that the output of the first query is not utilized at all. It is easy to verify that it is indeed satisfied by $+$ and $-$.

\paragraph{Advanced Compositional Properties}
One of the advantage of applying model checking is that we may get tighter bounds for compositional properties. Consider our survey example, and the property $\dpriv{1}{.5} (\X Y)$. Obviously, it holds in states $+$ and $-$. A careful check ensures that one cannot decrease the value of $\epsilon_1=1$ or $\delta_1=.5$ without increasing the other one. 


Now we consider the formula $\dpriv{\epsilon_2}{\delta_2} (\X (Y\wedge \X Y))$ under the model in ???. Applying the compositional theorem\lz{Bow-Yaw, as we only refer to the book, I wonder whether using the notion $e^\epsilon$ is easier to explain?}, one can choose $\epsilon_2=\epsilon_1^2=1$ and $\delta_2=2\delta_1=1$. However, we can check easily that one can get better privacy parameter $(1,0.5)$ or $(0,\frac{9}{16})$. 


\paragraph{Infinite ...}
Note that $\dpCTL$ generalizes differential privacy to infinite
behaviors naturally. Classical differential privacy discusses properties about
randomized terminating mechanisms. Sophisticated mechanisms for
streaming data have also been discussed in social networks and
internet of things. Such mechanisms inherently have infinite
computation. Their privacy guarantees are approximated by those of the
prefixes of computation.
With \dpCTL, the subformula $\phi$ in $\dpriv{\epsilon}{\delta} \phi$
can be an arbitrary $\dpCTL$ path formula. One can specify
$(\epsilon, \delta)$-closedness over infinite behaviors naturally. For instance,
$\dpriv{\epsilon}{\delta} (\G \PJ{[1,1]} (\F \mathit{high}))$ \lz{now we can use $G F high$}
specifies states having $(\epsilon, \delta)$-close
infinitely often $\mathit{high}$ behaviors to its neighbors.





\section{Markov Decision Processes}
The semantics over Markov chains generalizes to Markov decision
processes as well. Note that two states with different enabled actions
are trivially distinguishable; no privacy can be preserved. Without
loss of generality, we consider reactive Markov decision processes.
Let $M = (S, \Act, \wp, L)$ be a reactive Markov
decision process. Define the satisfaction
relation $M, \neighbor{S}, s \models \Phi$ as follows.
\begin{eqnarray*}
  M, \neighbor{S}, s \models \top\\
  M, \neighbor{S}, s \models p
  & \textmd{ if } &
  p \in L(s)\\
  M, \neighbor{S}, s \models \neg \Phi
  & \textmd{ if } &
  M, \neighbor{S}, s \not\models \Phi\\
  M, \neighbor{S}, s \models \Phi_0 \wedge \Phi_1
  & \textmd{ if } &
  M, \neighbor{S}, s \models \Phi_0 \textmd{ and }
  M, \neighbor{S}, s \models \Phi_1\\
  M, \neighbor{S}, s \models \PJ{J} \phi
  & \textmd{ if } &
  \myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} \in J
  \textmd{ for every scheduler } \scheduler{S}\\
  M, \neighbor{S}, s \models \dpriv{\epsilon}{\delta} \phi
  & \textmd{ if } &
  \textmd{for all } t \textmd{ with } s \neighbor{S} t \textmd{ and
   position-independent scheduler } \\
  & & \scheduler{S},
   \myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} \leq \epsilon \cdot
   \myPr{t}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} + \delta
   \textmd{ and }
   \myPr{t}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} \leq \epsilon \cdot
  \\
  & &
   \myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} + \delta
\end{eqnarray*}

\lz{how about using "query sequence" instead of position-independent scheduler, and then argue that it makes sense for the resulting MDP model from differential privacy? This corresponds to a special scheduler that is applied on both neighboring states. }
Recall that $M_{\scheduler{S}}$ is but a Markov chain. The semantics
of $M_{\scheduler{S}}, \neighbor{S}, \pi \models \phi$ and hence the
probability $\myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi}$ are
defined as in Markov chains.
The semantics of $\dpCTL$ on reactive Markov decision processes
is again standard except the differentially private operator
$\dpriv{\epsilon}{\delta}$. For any path formula $\phi$,
$\dpriv{\epsilon}{\delta} \phi$ specifies states which are $(\epsilon,
\delta)$-close to all its neighbors in having paths satisfying $\phi$
for arbitrary position-independent schedulers. That is, no
position-independent scheduler can force any of neighbors to have
probabilistically distinguishable behaviors.

\noindent
\emph{Discussion.}
Another definition of the differentially private operator
might be:
\begin{eqnarray*}
  M, \neighbor{S}, s \models \dpriv{\epsilon}{\delta}^{\mathit{bad}} \phi
  & \textmd{ if } &
  \textmd{for all } t \textmd{ with } s \neighbor{S} t \textmd{ and
  scheduler } \scheduler{S},
  \myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} \leq
  \epsilon \cdot\\
  && \myPr{t}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} + \delta \textmd{ and }
  \myPr{t}{M_{\scheduler{S}}}{\neighbor{S}}{\phi} \leq
  \epsilon \cdot \myPr{s}{M_{\scheduler{S}}}{\neighbor{S}}{\phi}
  + \delta
\end{eqnarray*}
A state satisfies $\dpriv{\epsilon}{\delta}^{\mathit{bad}} \phi$ if
no scheduler could differentiate the probabilities of paths satisfying
$\phi$ from neighbors. Such a definition would allow strategies to
take different actions from different states. It might need different
action sequences to distinguish neighbors. Two neighbors could still be
differentiated by a action sequence and lose their privacy. We hence
prefer the original definition.

\hide{
Since the scheduler attaining the maximally probable
behavior from a state may be different from the scheduler attaining
the maximally probable behavior from its neighbors, two
neighbors may still be distinguished by a scheduler. The
weaker definition does not preserve differential privacy. We hence
prefer the original definition.
}
