
The sensitivity of queries is required to apply the (truncated)
$\alpha$-geometric mechanism. Recall that the sensitivity 
is the maximal difference of query results on any two neighbors. 
Two practical problems may arise for
mechanisms depending on query sensitivity. First, 
sensitivity of queries can be hard to compute. Second,
the sensitivity over arbitrary neighbors can be too conservative for
the actual dataset in use. One therefore would like to have
mechanisms independent of query sensitivity.

Subsampling is a technique to design such mechanisms~\cite{DR:14:AFDP}. Concretely, let
us consider $\mathcal{X} = \{ R, B \}$ (for red and blue team members) and a
dataset $d \in \mathcal{X}^n$. Suppose we would like to ask which team
is the majority in the dataset while respecting individual
privacy. This can be achieved as
follows~(Algorithm~\ref{algorithm:subsampling-majority}). The
mechanism first samples $m$ 
sub-datasets $\hat{d}_1, \hat{d}_2, \ldots, \hat{d}_m$ from
$d$~(line~\ref{algorithm:subsampling-majority-subsample}). 
It then computes the majority of each sub-dataset and obtains $m$
sub-results. Let $\mathit{count}_R$ and $\mathit{count}_B$ be the
number of sub-datasets with the majority $R$ and $B$
respectively~(line~\ref{algorithm:subsampling-majority-count}). 
Since there are $m$ sub-datasets, we have
$\mathit{count}_R + \mathit{count}_B = m$. To ensure differential
privacy, the mechanism makes sure the difference $| \mathit{count}_R -
\mathit{count}_B |$ is significantly large after perturbation. In
line~\ref{algorithm:subsampling-majority-noise},
$\mathit{Lap}(p)$ denotes the continuous random
variable with the probability density
function $f(x) = \frac{1}{2p}e^{-|x|/p}$ of the Laplace distribution.
If the perturbed difference is sufficiently large,
the mechanism reports $1$ if the majority of the $m$
sub-results is
$R$ or $0$ if it is
$B$~(line~\ref{algorithm:subsampling-majority-output}). Otherwise,  no
information is 
revealed~(line~\ref{algorithm:subsampling-majority-mute}).

\begin{algorithm}
  \begin{algorithmic}[1]
    \Function{SubsamplingMajority}{$d$, $f$}
    \Require{$d \in \{ R, B \}^n$, $f : \{ R, B \}^* \rightarrow \{ R,
      B \}$}
    \State{$q, m \leftarrow 
            \frac{\epsilon}{64\ln (1/\delta)},
            \frac{\log (n/\delta)}{q^2}$}
    \State{Subsample $m$ data sets $\hat{d}_1, \hat{d}_2, \ldots,
      \hat{d}_m$ from $d$ where each row of $d$ is chosen with
      probability $q$}
    \label{algorithm:subsampling-majority-subsample}
    \State{$\mathit{count}_R, \mathit{count}_B \leftarrow 
            | \{ i : f (\hat{d}_i) = R \} |,
            | \{ i : f (\hat{d}_i) = B \} |$}
    \label{algorithm:subsampling-majority-count}
    \State{$r \leftarrow | \mathit{count}_R - \mathit{count}_B | /
      (4mq) - 1$}
    \label{algorithm:subsampling-majority-difference}
    \If{${r} + \mathit{Lap}(\frac{1}{\epsilon}) > \ln (1/\delta)/\epsilon$}
    \label{algorithm:subsampling-majority-noise}
      \State{\textbf{if} $\mathit{count}_R \geq \mathit{count}_B$
             \textbf{then} \Return { $1$ } \textbf{else} \Return {$0$} } 
      \label{algorithm:subsampling-majority-output}
    \Else
      \State{\Return {$\bot$}}
      \label{algorithm:subsampling-majority-mute}
    \EndIf
  \EndFunction
  \end{algorithmic}
  \caption{Subsampling Majority}
  \label{algorithm:subsampling-majority}
\end{algorithm}

Fix the dataset size $n$ and privacy parameters $\epsilon$, $\delta$,
the subsampling majority mechanism can be modeled by a Markov chain.
Figure~\ref{figure:subsampling-majority-markov-chain} gives a sketch
of the Markov chain for $n = 3$. The leftmost four states represent
all possible datasets. Given a dataset, $m$ samples are taken with
replacement. Outcomes of these samples are denoted by
$(\mathit{count}_R, \mathit{count}_B)$. There are only $m + 1$
outcomes: $(m, 0), (m - 1, 1), \ldots, (0, m)$. Each outcome is
represented by a state in
Figure~\ref{figure:subsampling-majority-markov-chain}. From each
dataset, the probability distribution on all outcomes gives the
transition probability. Next, observe that  $|
\mathit{count}_R - \mathit{count}_B |$ can have only finitely many
values. The values of $r$ 
(line~\ref{algorithm:subsampling-majority-difference}) hence belong to
a finite set $\{ r_m, \ldots, r_M \}$ with the minimum $r_m$ and
maximum $r_M$. 
%For each $r \in \{ r_m, \ldots, r_M \}$, it is represented by a state
%in Figure~\ref{figure:subsampling-majority-markov-chain}. 
For instance,
both outcomes $(m, 0)$ and $(0, m)$ transit to the state $r_M = 1/(4q)
- 1$ with probability $1$. For each $r \in \{ r_m, \ldots, r_M \}$,
the probability of having $r + \mathit{Lap}(\frac{1}{\epsilon}) > \ln
(1/\delta)/\epsilon$ (line~\ref{algorithm:subsampling-majority-noise})
is equal to the probability of $\mathit{Lap}(\frac{1}{\epsilon}) > \ln
(1/\delta)/\epsilon - r$. This is equal to $\int^{\infty}_{\ln
  (1/\delta)/\epsilon - r} \frac{\epsilon}{2}e^{-\epsilon|x|} dx$.
From each state $r \in \{ r_m, \ldots, r_M \}$, it hence goes to the state
$\top$ with probability $\int^{\infty}_{\ln (1/\delta)/\epsilon - r}
\frac{\epsilon}{2}e^{-\epsilon|x|} dx$ and to the state $\bot$ with
probability $1 - \int^{\infty}_{\ln (1/\delta)/\epsilon - r}
\frac{\epsilon}{2}e^{-\epsilon|x|} dx$. Finally, the
Markov chain moves from the state $\top$ to $1$ if
$\mathit{count}_R \geq \mathit{count}_B$; 
otherwise, it moves to $0$. Two dataset states are neighbors if they
differ at most one member. For example, $rrb$ is a neighbor
of $rrr$ and $rbb$ but not $bbb$.


\begin{figure}
  \centering
    \resizebox{.9\columnwidth}{!}{
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node
      distance=2cm,node/.style={circle,draw}]

      \node[node] (rrr) at (-5,  1.5) { $rrr$ };
%      \node at (-4.4, 2) { $\vdots$ };
      \node[node] (rrb) at (-5.5,  0.55) { $rrb$ };
%      \node at (-4.4, .75) { $\vdots$ };
      \node[node] (rbb) at (-5.5, -0.55) { $rbb$ };
%      \node at (-4.4, -.5) { $\vdots$ };
      \node[node] (bbb) at (-5, -1.5) { $bbb$ };
%      \node at (-4.4, -1.75) { $\vdots$ };

      \node[node] (rmb0) at (-2.5,  1.5) { $m,0$ };
      \node at (-2.5, 0) { $\vdots$ };
      \node[node] (r0bm) at (-2.5, -1.5) { $0,m$ };

      \node[node] (Rm) at (0,  1.5) { $r_m$ };
      \node at (0, 0) { $\vdots$ };
      \node[node] (RM) at (0, -1.5) { $r_M$ };

      \node[node] (top) at (2.5,  0.75) { $\top$ };
      \node[node] (bot) at (2.5, -0.75) { $\bot$ };

      \node[node] (outR) at (5,  1.25) { $1$ };
      \node[node] (outB) at (5, .25) { $0$ };

      \path
      (rrr) edge node [above] { } (rmb0)
      (rrr) edge node [above] { } (r0bm)
      (rrb) edge node [above] { } (rmb0)
      (rrb) edge node [above] { } (r0bm)
      (rbb) edge node [above] { } (rmb0)
      (rbb) edge node [above] { } (r0bm)
      (bbb) edge node [above] { } (rmb0)
      (bbb) edge node [above] { } (r0bm)

      (rmb0) edge node [above] {} (RM)
      (r0bm) edge node [above] {} (RM)

      (Rm) edge node [above] {} (top)
      (Rm) edge node [above] {} (bot)
      (RM) edge node [above] {} (top)
      (RM) edge node [above] {} (bot)

      (top) edge (outR)
      (top) edge (outB)
      ;

      \end{tikzpicture}
    }
  \caption{Markov Chain for Subsampling Majority}
  \label{figure:subsampling-majority-markov-chain}
\end{figure}

